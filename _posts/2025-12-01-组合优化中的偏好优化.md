---
layout: post
title: "组合优化中的偏好优化"
date: 2025-12-01
categories: [SIGPD]
tags: [组合优化, NCO, 偏好优化]
---

# 组合优化中的偏好优化

## 一、组合优化问题

### 1、问题定义

组合优化问题是对数学方法的研究寻找离散事件的最优编排、分组、次序或筛选，是一类重要的优化问题。实际应用中，许多资源调度、生成、物流以及军事领域的问题都可以建模成组合优化问题，常见的需要组合优化来求解的问题包括01背包、旅行商问题等等

一般来说，组合优化问题的数学形式如下所示：

$\begin{aligned}
&\min_{x \in \mathcal{X}}  && f(x) \\
&\text{subject to} && g_j(x) \le 0,\quad j \in \mathcal{J},\\
&                  && h_k(x) = 0,\quad k \in \mathcal{K}.
\end{aligned}$

其中，$x$是一个$n$维的向量，代表决策变量;$\mathcal{X}$是可行域，$f(x)$是要最小化的目标函数，$g$和$h$分别代表了不等式和等式约束

### 2、组合优化与机器学习

目前，组合优化与机器学习方法相结合已经成为了人工智能时代前沿的交叉研究方向。通常来说，机器学习方法求解组合优化问题包括监督学习、强化学习以及图神经网络。它们能从同分布的数据中学习到通用的规律，能够快速求解同一类问题。使用不同的机器学习方法可以解决不同的组合优化问题，例如图优化和离散优化等等。

![image-20251201164929194](/figures/组合优化.resources/all-1.png)

## 二、神经组合优化

### 1、相关工作

在真实的组合优化问题中，硬约束（任何违反都使解直接不可行）难以处理

- **传统神经求解器的做法：**
  - Masking策略：结构复杂、随规模迅速失效，无法应对复杂/全局/非线性约束
  - 拉格朗日惩罚：需要人工调参、极不稳定
- **组合优化求解方法分类：**

（1）**End-to-end constructive methods：**通过自回归方式一步步构造解，Attention Model（Kool 2019）、POMO（Kwon 2020）、LEHD、InViT（大规模问题）、BOPO/POCO（偏好优化用于NCO）

- 训练端到端，推理速度快
- 在标准TSP/CVRP等基准上表现很强
- <font color="red">大多数依赖mask实现可行性，遇到复杂约束时难以适用</font>

> 自回归生成
>
> 典型组合优化问题（TSP、VRP、Job Shop、背包、调度…）都可以想成“从一个有限集合里按顺序做选择”
>
> - 以TSP为例：
>   - 状态：已经走过的城市序列$s_{t} = (i_{1}, \ldots, i_{t})$
>   - 动作：在剩余城市中选一个$a_t \in \{1,\ldots,n\} \setminus \{i_1,\ldots,i_t\}$
>   - 直到长度n停止，得到完整解$\pi = (i_1,\ldots,i_n)$
>
> 标准自回归过程：$p(\pi \mid x)
> = \prod_{t=1}^{n} p_\theta(a_t \mid a_{<t}, x)$
>
> - $x$: 问题实例（点坐标、图结构、约束等）
> - $a_t$: 第 $t$ 步决策
> - $p_\theta$: 用神经网络（Transformer）建模的条件分布
>
> ![image-20251201171350229](/figures/组合优化.resources/all-2.png)

（2）**Interative improvement methods：**首先生成一个解，再通过局部搜索反复改进，DACT（2021）、NeuOpt（2023）

- 可以跳出局部最优
- <font color="red">推理成本高，面对复杂全局约束仍然困难</font>

（3）**Non-autoregressive methods：**不按顺序构造，一次性生成解，DeepACO（蚁群+深度学习）、DIFUSCO

- 推理快，有潜力生成多样解
- <font color="red">难以控制每一步的可行性，对复杂约束仍需额外机制</font>

### 2、偏好优化

**直接偏好优化（Direct Preference Optimization）：**

> Direct preference optimization: Your language model is secretly a reward model.https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf]

传统RLHF流程：

1、奖励建模：基于人类偏好数据训练奖励函数$r(x,y)$；

2、策略优化：使用强化学习算法最大化奖励，同时通过KL散度约束防止策略偏离参考模型$\pi_{ref}$

直接偏好优化突破性：

1、消除奖励建模阶段：通过变量变换，将奖励函数表示为最优策略$\pi^∗$和参考策略$\pi_{ref}$的函数：

$r(x,y) = \beta \log \frac{\pi^{*}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}+\beta \log Z(x)$

2、直接优化偏好损失：构建二元偏好数据$D = \{(x, y_w, y_l)\}$，损失函数定义：

$\mathcal{L}_{\mathrm{DPO}}
= -\mathbb{E}_{(x,y_w,y_l)\sim D}
\left[
\log \sigma\!
\left(
\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)}
-
\beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}
\right)
\right]$

该目标直接最大化偏好响应对的似然概率

**组合优化中的偏好优化（Preference Optimization Methods）：**

给定实例$x \in \mathcal{X}$，带参数$\theta$的策略网络，策略$π_θ(⋅|x)$生成$N$个候选解：$\mathcal{T}_x = \{ \tau_i \}_{i=1}^{N} \sim \pi_\theta(\cdot \mid x)$

对每个对 $(\tau_i, \tau_j)$，定义：$y_{ij} = \mathbf{1}\{ f(x,\tau_i) < f(x,\tau_j) \}$

Bradley-Terry模型转换：$p_\theta(\tau_i > \tau_j \mid x)
= \frac{e^{r(\tau_i)}}{e^{r(\tau_i)} + e^{r(\tau_j)}}
= \sigma\!\left( r(\tau_i) - r(\tau_j) \right)
= \sigma\!\left( \beta \cdot [\log \pi_\theta(\tau_i \mid x) - \log \pi_\theta(\tau_j \mid x)] \right)$

最大化所有偏好对的似然就是最小化损失：$\mathcal{L}(\theta)
= - \mathbb{E}_{x \sim \chi}
\left[
\frac{1}{|\mathcal{T}_x|^2}
\sum_{i \ne j} 
y_{ij} \log p_\theta(\tau_i > \tau_j \mid x)
\right]$

将 BT 模型代入后的损失最终形式：$\mathcal{L}(\theta)
= - \mathbb{E}_{x \sim \chi}
\left[
\frac{1}{|\mathcal{T}_x|^2}
\sum_{i \ne j} 
y_{ij}
\log \sigma\!\left(
\beta [\log \pi_\theta(\tau_i \mid x) - \log \pi_\theta(\tau_j \mid x)]
\right)
\right]$

## 三、相关文献

> Preference Optimization for Combinatorial Optimization Problems 组合优化问题的偏好优化（ICML2025）(https://arxiv.org/pdf/2505.08735?)
>
> BOPO: Neural Combinatorial Optimization via Best-anchored and  Objective-guided Preference Optimization BOPO：基于最佳锚定和目标引导的偏好优化的神经组合优化（ICML2025）(https://arxiv.org/pdf/2503.07580)
>
> UCPO: A Universal Constrained Combinatorial Optimization Method via Preference Optimization UCPO：一种通过偏好优化的通用约束组合优化方法（AAAI2026）(https://arxiv.org/pdf/2511.10148)

### 1、Preference Optimization for Combinatorial Optimization Problems (ICML2025)

组合优化问题的偏好优化

Abstract：强化学习（RL）已成为神经组合优化的强大工具，使模型能够学习解决复杂问题的启发式方法，而不需要专家知识。尽管取得了重大进展，但现有的RL方法仍面临着挑战，例如奖励信号减少以及在巨大的组合动作空间中探索效率低下，从而导致效率低下。本文提出了偏好优化，这是一种通过统计比较建模将量化奖励信号转化为定性偏好信号的新颖方法，强调了采样解决方案之间的优越性。从方法学上来说，通过在政策方面重新参数化奖励函数并利用偏好模型，我们制定了一个熵正规化RL目标，该目标将政策直接与偏好保持一致，同时避免了棘手的计算。此外，我们将本地搜索技术集成到微调中而不是后处理中，以生成高质量的偏好对，帮助策略摆脱局部最优。对旅行推销员问题（TSP）、容量车辆路径问题（CVRP）和灵活流水车间问题（FFSP）等各种基准的经验结果表明，我们的方法显着优于现有的RL算法，实现了卓越的收敛效率和解质量。

#### 1.1方法

##### Challenge

**传统强化学习目标：**$\max_{\pi_\theta} \; \mathbb{E}_{x \sim \mathcal{D}}
\left[ \mathbb{E}_{\tau \sim \pi_\theta(\cdot \mid x)}
\big[ r(x,\tau) \big] \right]$	

- $x \sim \mathcal{D}$:问题实例
- $\tau \sim \pi_\theta(\cdot \mid x)$：策略生成的解轨迹
- $r(x,\tau)$：轨迹奖励

即通过调整策略参数 $\theta$ 来最大化从分布$\mathcal{D}$中采样的实例$x$以及从策略中采样的轨迹$\tau$所获得的期望奖励。

**传统强化学习使用的梯度更新策略：**$\nabla_\theta J(\theta)= \mathbb{E}_{x \sim \mathcal{D},\, \tau \sim \pi_\theta(\tau \mid x)}
\left[ \left( r(x,\tau) - b(x) \right)
\nabla_\theta \log \pi_\theta(\tau \mid x) \right]$ $\approx
\frac{1}{|\mathcal{D}|}
\sum_{x \in \mathcal{D}}
\frac{1}{|S_x|}
\sum_{\tau \in S_x}
\left[ \left( r(x,\tau) - b(x) \right)
\nabla_\theta \log \pi_\theta(\tau \mid x) \right]$

$r(x,\tau)-b(x)$存在两个挑战：

- 探索不足：策略可能过早收敛到局部最优，缺乏对解空间的充分探索
- 奖励衰减：策略改进后，优势值$A(x,\tau)=r(x,\tau)−b(x)$幅度趋近于零，导致梯度消失。

##### 整体思路

![image-20251201145958837](/figures/组合优化.resources/POCO-framework.png)

- 解采样模块：通过策略网络生成候选解，策略网络通常是一个编码器-解码器模块，能够直接输出选择每个动作的概率分布，整个轨迹的概率是各个步骤概率的乘积。
- 偏好比较模块：构建偏好标签，将定量奖励转换成定性奖励，原本只优化一个解，变成比较两个解之间的好坏
- 局部搜索微调模块：可选操作，用来提升解的质量，帮助策略逃离局部最优
- 最后根据公式计算梯度，更新策略参数$\theta$

##### 策略熵

策略熵：$\mathcal{H}(\pi_\theta)
= - \sum_{\tau} \pi_\theta(\tau \mid x)\, \log \pi_\theta(\tau \mid x)$

- 熵的说明：![image-20251201150739564](/figures/组合优化.resources/POCO-1.png)

加入熵正则项后的强化学习目标：$\max_{\pi_\theta} \;
\mathbb{E}_{x \sim \mathcal{D}}
\left[
\mathbb{E}_{\tau \sim \pi_\theta(\cdot \mid x)}
\big[ r(x,\tau) \big]
\;+\;
\alpha \mathcal{H}\!\left( \pi_\theta(\cdot \mid x) \right)
\right]$

<font color="red">直接计算$\mathcal{H}$需要枚举所有轨迹$\tau$，导致组合空间太大，计算不可行</font>

由策略熵$\mathcal{H}(\pi_\theta)
= - \sum_{\tau} \pi_\theta(\tau \mid x)\, \log \pi_\theta(\tau \mid x)$采用变分法推导最优策略闭式解$\pi^{*}(\tau \mid x)
= \frac{1}{Z(x)} \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right)$，其中$Z(x) = \sum_{\tau} \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right)$是归一化常数

> **推导过程：**
>
> 总体思想：只看固定一个$x$时的最优策略$\pi(\tau \mid x)$，把它当作在$\tau$上的一个离散分布$\{\pi_\tau\}_\tau$
>
> **熵正则的目标**是（省略对$x$的期望，只看单个$x$）:$J_x(\pi)
> = \mathbb{E}_{\tau\sim\pi(\cdot|x)}[r(x,\tau)]+\alpha \mathcal{H}(\pi)$，其中$
> \mathcal{H}(\pi)
> = -\sum_{\tau} \pi(\tau\mid x)\,\log \pi(\tau\mid x).$
>
> 写成求和形式：$J_x(\pi)= \sum_{\tau}\pi(\tau|x)\, r(x,\tau)+\alpha (-\sum_{\tau}\pi(\tau|x)\,\log \pi(\tau|x))=\sum_{\tau}\pi(\tau|x)\, r(x,\tau)-\alpha\sum_{\tau}\pi(\tau|x)\, \log\pi(\tau|x).$约束：$\sum_{\tau}\pi_\tau=1,\pi_\tau \geq0.$
>
> **构造拉格朗日函数**：用拉格朗日乘子 $\lambda$处理归一化约束：$\mathcal{L}(\pi,\lambda)
> = \sum_{\tau}\pi(\tau|x)\, r(x,\tau)-\alpha \sum_{\tau}\pi(\tau|x)\log \pi(\tau|x)+\lambda\left( \sum_{\tau}\pi(\tau|x) - 1 \right).$
>
> 对每个$\pi_\tau$做变分，即求偏导并令其为0：$\frac{\partial \mathcal{L}}{\partial \pi(\tau|x)}= r(x,\tau)-\alpha(\log\pi(\tau|x) + 1)+\lambda = 0.$整理得$\log\pi(\tau|x)= \frac{1}{\alpha}\left( r(x,\tau) + \lambda - \alpha \right).$
>
> 两边取指数：$\pi(\tau|x)= \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right)  \cdot \exp\!\left( \frac{\lambda - \alpha}{\alpha} \right).$注意到$\exp\!\left( \frac{\lambda - \alpha}{\alpha} \right)$与$\tau$无关，是一个常数，记为$\frac{1}{Z(X)}$,所以$\pi^{*}(\tau|x)
> = \frac{1}{Z(x)} \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right).$
>
> 归一化常数$Z(x)$:由归一化约束$\sum_{\tau}\pi_\tau=1$得到$1=\sum_{\tau}\pi^{*}(\tau|x)
> = \frac{1}{Z(x)} \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right)$ ,即$Z(x)= \sum_{\tau} \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right).$

<font color="red">计算$Z(x)$需要枚举所有轨迹，仍然不可行</font>

对最优策略闭式解$\pi^{*}(\tau \mid x)
= \frac{1}{Z(x)} \exp\!\left( \frac{1}{\alpha} r(x,\tau) \right)$两边取指数$\hat{r}(x,\tau) = \alpha \log \pi(\tau \mid x) + \alpha \log Z(x)$

> 关键性质：若奖励函数偏移实例相关函数$h(x)$（即$\hat{r}^{’}(x,\tau)=\hat{r}(x,\tau)-h(x)$），则诱导策略不变。
>
> 推论：$Z(x)$作为常数偏移量（$h(x)=-\alpha \log Z(x)$）,不影响策略的偏好顺序，可忽略

重参数化后的奖励函数：$\hat{r}(x,\tau) = \alpha \log \pi(\tau \mid x) + \alpha \log Z(x)$

使用偏好模型关联奖励差与偏好概率：$p^{*}(\tau_1 \succ \tau_2)= f(\hat{r}(x,\tau_1) - \hat{r}(x,\tau_2))$ , $\hat{r}(x,\tau_1) - \hat{r}(x,\tau_2)= \alpha\left[ \log \pi(\tau_1 \mid x) - \log \pi(\tau_2 \mid x) \right]$

**最终形式：$p(\tau_1 \succ \tau_2 \mid x)= f\!\left( \alpha\left[\log \pi(\tau_1 \mid x)      - \log \pi(\tau_2 \mid x)\right] \right)$**

策略优化目标：$\max_{\theta}\;
\mathbb{E}_{x\sim \mathcal{D},\, \tau \sim \pi_\theta}
\left[
\mathbf{1}(r(\tau_1) > r(\tau_2)) \cdot
\log p_\theta(\tau_1 > \tau_2 \mid x)
\right]$

以Bradley-Terry模型为例的最终梯度形式：$\nabla_\theta J(\theta)
\approx
\frac{\alpha}{|\mathcal{D}|\, |S_x|^2}
\sum_{x \in \mathcal{D}}
\sum_{\tau \in S_x}
\sum_{\tau' \in S_x}
\left[
\big(g_{BT}(\tau, \tau', x)-g_{BT}(\tau', \tau, x)\big)
\nabla_\theta \log \pi_\theta(\tau \mid x)
\right]$

- $\big(g_{BT}(\tau, \tau', x)-g_{BT}(\tau', \tau, x)\big)$:梯度更新强化优质解的概率，弱化劣质解

##### 局部搜索的微调（Fine-Tuning with Local Search）

**背景与动机：**

- 神经求解器的局限性：
  - 神经求解器虽推理高效，但常陷入局部最优解，难以达到接近最优的解。
  - 传统方法将LS作为后处理步骤（如对训练好的模型生成解进行优化），但会增加额外推理时间。
- RL方法的瓶颈：
  - 传统RL算法（如REINFORCE）依赖数值奖励信号，而LS对解的改进可能微小（如路径长度减少0.1%），导致梯度信号微弱，无法有效更新策略。

**流程：**

1、生成改进解

- 对策略采样的解$\tau$执行少量LS迭代，得到改进解$LS(\tau)$
- 改进解满足$r(x,LS(\tau))\geq r(x,\tau)$

2、构建偏好对

- 创建偏好三元组$(\tau,LS(\tau),y)$

3、微调目标函数

- 优化目标转换为最大化改进解优于原解的对数概率:$\max_{\theta}\;
  \mathbb{E}_{x\sim\mathcal{D},\, \tau \sim \pi_\theta}
  \left[y \cdot \log p_\theta\big( LS(\tau) > \tau \mid x \big)
  \right]$
- 代入偏好模型后，目标函数具体化为：$\mathbb{E}_{x,\tau}\left[f\big(\alpha \big[\log \pi_\theta( LS(\tau) \mid x)-\log \pi_\theta(\tau \mid x)\big]\big)\right]$

**优势：**

- 避免推理开销：$LS$仅在训练阶段调用，不增加推理时间（传统$LS$后处理需在每次推理时执行）。实验表明，少量$LS$迭代（如5-10次）足以生成高质量偏好对，计算开销可控。
- 解决策略偏移问题：$LS$生成的改进解 $LS(\tau)$可能偏离当前策略分布（off-policy）。PO框架通过偏好对齐（而非奖励值匹配）自然规避该问题：将 $LS(\tau)$ 视为“专家示范”，策略学习模仿其优越性而非拟合数值奖励（类似模仿学习）。
- 突破局部最优：$LS$引导策略探索更优解区域，帮助逃离训练收敛后的局部最优。实验显示，微调后POMO在TSP100上差距从0.07%→0.03%，CVRP100从1.37%→1.19%

#### 1.2实验

##### 实验设置

- Benchmark：
  - TSP: N=20,50,100 （路径组合优化）
  - CVRP: N=20,50,100 （多目标路径+资源限制）
  - FFSP: 20/50/100任务 （时间调度+并发资源冲突）
- Baseline
  - 传统求解器
    - 精确求解器：Concorde (TSP), CPLEX (FFSP)
    - 启发式求解器：LKH3 (TSP/CVRP), HGS (CVRP)
  - 神经求解器
    - End-to-End方法：AM、POMO、Sym-NCO、Pointerformer、MatNet、DIMES
    - 混合方法：NeuroLKH
  - 监督学习：BQ-NCO、DIFUSCO、T2TCO
- 实验环境：NVIDIA 24GB-RTX 4090 GPUs and an Intel Xeon Gold 6133 CPU

##### 实验结果

- PO和RF在POMO、Sym-NCO、Pointerformer上的收敛速度比较

![image-20251201144146424](/figures/组合优化.resources/POCO-Experiment1.png)

- 在推理时间相同的情况下，用 PO 训练的模型在解质量方面大多优于用 RF 训练的模型

![image-20251201144242954](/figures/组合优化.resources/POCO-Experiment2.png)

- FFSP上的解质量比较

![image-20251201144320029](/figures/组合优化.resources/POCO-Experiment3.png)



### 2、BOPO: Neural Combinatorial Optimization via Best-anchored and  Objective-guided Preference Optimization (ICML2025)

BOPO：基于最佳锚点和目标引导偏好优化的神经组合优化

Abstract：神经组合优化（NCO）已成为解决NP难问题的一种有前途的方法。然而，由于奖励稀疏和解未充分利用，流行的基于RL的方法存在样本效率低的问题。我们提出组合优化（POCO）的偏好优化，这是一种通过目标值利用解决方案偏好的训练范式。它介绍了：（1）高效的偏好对构建，用于更好地探索和利用解，以及（2）新颖的损失函数，通过客观差异自适应地缩放梯度，消除对奖励模型或参考政策的依赖。关于Job-Shop Pluting（JSP）、Traveling Sales（TSP）和Flexible Job-Shop Pluting（FJSP）的实验表明，POCO优于最先进的神经方法，通过高效的推理大幅缩小了最优性差距。POCO是架构不可知的，能够与现有NCO模型无缝集成，并将偏好优化建立为组合优化的原则框架。

#### 2.1方法

##### 整体思路

![image-20251201125829406](/figures/组合优化.resources/BOPO-framework.png)

基于两个观察：

- NCO模型（通常是生成模型）可以为给定问题实例生成多个不同的解
- 可以以低成本计算COP解的目标值

如上图所示，BOPO包括两个基本部分，根据采样解构建多个偏好对和构建偏好优化损失。

- 使用NCO偏好优化的新型训练范式
- 为COP设计了偏好对构建方法，以更好地探索和利用解
- 设计由目标值量化的偏好优化损失

#### 2.2实验

##### 实验设置

- Benchmark

  - 作业车间调度问题（JSP）

  - 旅行商问题（TSP）

  - 柔性作业车间调度问题（FJSP）

- Baseline

  - TSP:Gurobi、LKH3、POMO、SLIM
  - JSP:Non-constructive、Greedy Constructive、Sampling Constructive
  - FJSP:Greedy Constructive、Sampling Constructive

- 实验环境：NVIDIA TITAN Xp GPU and Intel(R) Xeon(R) E5-2680 CPU.

##### 实验结果

- TSP：

![image-20251201112938993](/figures/组合优化.resources/BOPO-Experiment1.png)

![image-20251201112958453](/figures/组合优化.resources/BOPO-Experiment2.png)

- JSP：

![image-20251201113049964](/figures/组合优化.resources/BOPO-Experiment3.png)

- FJSP:

![image-20251201113129951](/figures/组合优化.resources/BOPO-Experiment4.png)

### 3、UCPO: A Universal Constrained Combinatorial Optimization Method via Preference Optimization (AAAI2026)

UCPO：一种通过偏好优化的通用约束组合优化方法（AAAI25）

Abstract：神经求解器在组合优化中已经展现出卓越表现，常常在速度、解质量和泛化能力方面超过传统启发式方法。然而，当面对无法通过简单 mask 机制有效处理的复杂约束时，其性能会显著下降。为了解决这一问题，我们提出 **Universal Constrained Preference Optimization（UCPO）** —— 一种全新的即插即用框架，能够通过专门设计的损失函数，将偏好学习无缝整合到现有神经求解器中，而无需修改模型结构。UCPO 将“满足约束”直接嵌入偏好目标中，从而消除对精细超参数调节的需求。借助轻量级的 warm-start 微调流程，UCPO 使得预训练模型在复杂约束任务中也能稳定地产生接近最优且可行的解，只需原始训练预算的 1%。

#### 3.1方法

概念：

- 对偶问题提供了原始问题的上下界（弱对偶性和强对偶性）

  - 弱对偶性：对于一个最小化问题，任何可行的对偶解的目标值都不会比原始问题的最优解更好

    最小化原问题的最优值≥最大化对偶问题的最优值

  - 强对偶性：指在特定条件下，原始问题的最优解和对偶问题的最优解是相等的。强对偶性确保了对偶问题不仅是原始问题的下界，还可以为原始问题提供一个与最优解相同的值。

    最小化原问题的最优值=最大化对偶问题的最优值



##### **整体思路**

![image-20251201105107532](/figures/组合优化.resources/ucpo-framework.png)

把一个有硬约束（不满足硬约束的解一定为不可行解）的组合优化问题，改写成一个偏好学习问题，再设计一个统一的loss，让原来的NCO policy通过“谁更好”的偏好信号，逐步学会：

1、先变得可行（满足约束）

2、再在可行集合中变得更优（目标函数更小）

具体分三步：

1、给每条轨迹打上“可行/不可行”标签，建立一个带约束的偏序关系；

2、用Bradley-Terry模型把这个偏序转成概率形式的偏好；

3、在这个偏好概率上构造三个Loss：

- Dual Explration Loss $L^{Dual}$
- Feasibility Margin Loss $L^{Margin}$
- Primal Refinement Loss $L^{Primal}$



##### **带约束的偏序关系如何构造？**

1、指示函数$I(x,\tau)$：给定一个实例$x$，约束集合写成：不等式约束$g_j(x,\tau)\leq0,j\in J$，等式约束$h_k(x,\tau)=0,k\in K$，为了区分解是否可行，定义可行性指示函数：

$I(x,\tau) =
  \begin{cases}
    1, & \exists j: g_j(x,\tau) > 0 \;\; \text{或} \;\; \exists k: h_k(x,\tau) \neq 0,\\[3pt]
    0, & \forall j: g_j(x,\tau) \le 0 \;\; \text{且} \;\; \forall k: h_k(x,\tau) = 0.
  \end{cases}$

其中 $I(x,\tau)=0$ 表示解完全满足所有约束，为可行解；$I(x,\tau)=1$ 表示至少违反一个约束，为不可行解。

2、为了同时刻画目标值与约束违反程度，引入拉格朗日函数
$
  L(x,\tau,\lambda,\mu)  =  f(x,\tau)  +  \sum_{j} \lambda_j g_j(x,\tau)  +  \sum_{k} \mu_k h_k(x,\tau),
$
其中 $\lambda_j,\mu_k \ge 0$ 被视作固定的惩罚系数。$L$ 越小表示目标值越优且约束违反程度越低。

作为一个标量评分，即考虑目标值，又考虑约束违反；经典做法：目标+约束范围*权重

**3、带约束的偏序$\tau_i>\tau_j$的三种情况：**

- 两条都可行（$I=0$），比目标值$f$

$I(x,\tau_i)=I(x,\tau_j)=0
    \quad \text{且} \quad
    f(x,\tau_i) < f(x,\tau_j),$ 则定义 $\tau_i \succ \tau_j$。可行解之间只比较原始目标值。

- 一条可行、一条不可行：可行的必然更好

$ I(x,\tau_i)=0,\quad I(x,\tau_j)=1,$则定义$\tau_i \succ \tau_j,$即任何可行解都优于不可行解。

- 两条都不可行，比较Lagrangian

$I(x,\tau_i)=I(x,\tau_j)=1
    \quad \text{且} \quad
    L(x,\tau_i,\lambda,\mu)
    <
    L(x,\tau_j,\lambda,\mu),
$
  则定义 $\tau_i \succ \tau_j$。此时在不可行区域内部，根据拉格朗日值区分“违反更少且目标更好”的解。

**三个分支实则分级排序，先看是否可行，可行之间看目标值，不可行之间看Lagrangian**

利用偏序关系，把同一个实例上采样到的所有轨迹{$\tau_i$}排序，$  \tau_1 \succ \tau_2 \succ \cdots \succ \tau_N.$分成两个集合

$  T^T &= \{\tau \in T_x \mid I(x,\tau)=0\}, \quad \text{（可行解集合）} \\
  T^F &= \{\tau \in T_x \mid I(x,\tau)=1\}, \quad \text{（不可行解集合）}$



##### Bradley-Terry模型与偏好概率

给定策略网络参数 $\theta$，策略为 $\pi_\theta(\tau \mid x)$。将轨迹的“效用”定义为$u_\theta(x,\tau) = \beta \log \pi_\theta(\tau \mid x),$
其中 $\beta > 0$ 是缩放因子。

根据 Bradley-Terry 模型，轨迹 $\tau_i$ 优于 $\tau_j$ 的偏好概率为:
$
  p_\theta(\tau_i \succ \tau_j \mid x)
  =
  \sigma\Big(
    u_\theta(x,\tau_i)
    -
    u_\theta(x,\tau_j)
  \Big)
  =
  \sigma\Big(
    \beta \big[
      \log \pi_\theta(\tau_i \mid x)
      -
      \log \pi_\theta(\tau_j \mid x)
    \big]
  \Big),
$
其中 $\sigma(z) = 1/(1 + e^{-z})$ 为 Sigmoid 函数。



##### 统一的成对偏好损失形式

对任意一对满足偏序关系的轨迹 $(\tau^{\text{好}}, \tau^{\text{差}})$，理想情况下有$p_\theta(\tau^{\text{好}} \succ \tau^{\text{差}} \mid x) \approx 1.$
对应的负对数似然损失为
$  \ell(\theta; x, \tau^{\text{好}}, \tau^{\text{差}})=-\log p_\theta(\tau^{\text{好}} \succ \tau^{\text{差}} \mid x)
=-\log \sigma\Big(
\beta \big[
  \log \pi_\theta(\tau^{\text{好}} \mid x)
  -
  \log \pi_\theta(\tau^{\text{差}} \mid x)
\big]
  \Big).
$
UCPO 的三个子损失（Dual, Margin, Primal）都是在此统一形式下，通过选择不同的轨迹对 $(\tau^{\text{好}}, \tau^{\text{差}})$ 以及不同的 $\beta$ 构造而来。

**1、$L^{Dual}$：在全不可行区域里往更少约束违反的方向爬(不可行区域探索)**

当当前采样中没有任何可行解，即$T^T = \emptyset$，只能在不可行集合 $T^F$ 内部区分“更少违反”的解。
首先选出不可行集合中拉格朗日值最小的轨迹
$  \tau^\circ  =  \arg\min_{\tau \in T^F}  L(x,\tau,\lambda,\mu).$
对于每个 $\tau \in T^F \setminus \{\tau^\circ\}$，我们都有偏好关系$  \tau^\circ \succ \tau.$
于是 Dual Exploration Loss 定义为:
$  L_{\text{Dual}}(\theta)  =-\mathbb{E}_{x}\frac{1}{|T^F|-1}  \sum_{\tau \in T^F \setminus \{\tau^\circ\}}  \log \sigma\Big(\beta_{\text{Dual}}\big[  \log \pi_\theta(\tau^\circ \mid x)  -  \log \pi_\theta(\tau \mid x)\big]  \Big),$
其中一种典型的缩放因子设定为
$\beta_{\text{Dual}}=\frac{L(x,\tau,\lambda,\mu)} {L(x,\tau^\circ,\lambda,\mu)}\ge 1,$
即越“糟糕”的不可行解（$L$ 越大），其梯度权重越大，从而被更强烈地压制。

同时，因为只有在「没有可行解」时启用这个 loss，它扮演的是一个**早期探索器**：先在不可行区域里朝「违反更少的方向」走，直到开始采到可行解。

**2、$L^{Margin}$：把可行解整体拉高到不可行解之上**

当采样中既有可行解又有不可行解，即$  T^T \neq \emptyset,  \qquad  T^F \neq \emptyset,$我们希望“最优可行解”整体上压过所有不可行解。
先在可行集合中选出目标值最好的轨迹$  \tau^\star  =  \arg\min_{\tau \in T^T}  f(x,\tau).$
对于每个不可行轨迹 $\tau \in T^F$，有偏好关系$  \tau^\star \succ \tau.$
Feasibility Margin Loss 定义为:$  L_{\text{Margin}}(\theta)  =-\mathbb{E}_{x}\frac{1}{|T^F|}  \sum_{\tau \in T^F}
  \log \sigma\Big(\beta_{\text{Margin}}\big[  \log \pi_\theta(\tau^\star \mid x)  -  \log \pi_\theta(\tau \mid x)
\big]  \Big),$
其中缩放因子可以设置为$  \beta_{\text{Margin}}  =  \frac{L(x,\tau,\lambda,\mu)}{f(x,\tau^\star)}  \;>\; 1.$
这样违反越严重、目标越差的不可行解，其对应的梯度越大，有利于将概率质量推向可行区域。

**3、$L^{Primal}$:在可行集合内部作细致优化**

当可行解数量大于等于 2，即$  |T^T| \ge 2,$我们在可行集合内部继续优化目标值。仍然以最优可行解
$  \tau^\star  =  \arg\min_{\tau \in T^T}  f(x,\tau)$
作为 anchor，对于每个其他可行轨迹$\tau \in T^T \setminus \{\tau^\star\}$，都有偏好关系$  \tau^\star \succ \tau.$
Primal Refinement Loss 定义为: $ L_{\text{Primal}}(\theta)  =-\mathbb{E}_{x}
\frac{1}{|T^T|-1}
  \sum_{\tau \in T^T \setminus \{\tau^\star\}}
  \log \sigma\Big(
\beta_{\text{Primal}}
\big[
  \log \pi_\theta(\tau^\star \mid x)
  -
  \log \pi_\theta(\tau \mid x)
\big]
  \Big),
$
其中缩放因子可以选为$  \beta_{\text{Primal}}  =  \frac{f(x,\tau)}{f(x,\tau^\star)}
  \;\;\ge 1.$
可行解中目标越差（$f$ 越大）的轨迹，对应的梯度权重越大，从而强化对最优可行解的偏好。

4、**总损失$L(\theta)$与自动切换机制：**

最终的训练目标由上述三部分损失加权求和：
$  L(\theta)  =  L_{\text{Dual}}(\theta)  +  L_{\text{Margin}}(\theta)  +  L_{\text{Primal}}(\theta).$
实际训练时，根据 $T^T$ 与 $T^F$ 的状态自动决定各项是否启用：

- 若 $T^T = \emptyset$，仅 $L_{\text{Dual}}$ 有效；
- 若 $T^T \neq \emptyset$ 且 $T^F \neq \emptyset$，$L_{\text{Margin}}$ 有效；
-  若 $|T^T| \ge 2$，$L_{\text{Primal}}$ 有效。

这样，UCPO 在训练过程中自然经历“从不可行区域探索 $\rightarrow$ 推向可行区域 $\rightarrow$ 在可行区域内部精细优化”的完整过程。

#### 3.2实验

##### 实验设置

- Benchmark：带时间窗的旅行商问题TSPTW、带吃水限制的旅行商问题TSPDL、带时间窗的容量车辆路径规划问题CVRPTW、带时间窗和车辆数量限制的车辆路径规划问题CVRPTWLV
- Baseline：

（1）问题特定的启发式方法：LKH3（Helsgaun 2017）

（2）通用的RLNCO：POMO（Kwon 2020）SLIM（Corsini 2024）

（3）约束专门化模型：PIP（Bi 2024）、PIP-D3

- 实验环境：NVIDIA GeForce  GTX 1080 Ti GPUs and Intel(R) Xeon(R) CPU E5-2640 v4  @ 2.40GHz CPUs.

##### 实验结果

![image-20251201111441523](/figures/组合优化.resources/UCPO-Experiment1.png)

![image-20251201111601728](/figures/组合优化.resources/UCPO-Experiment2.png)

![image-20251201111631766](/figures/组合优化.resources/UCPO-Experiment3.png)

## 四、总结

- 当前（2024-2025）神经组合优化领域热点
  - 偏好优化/RLHF类强化学习用于组合优化
  - 基于预训练模型的快速微调
  - 通过采样+reranking/fine-grained调整

- 传统NCO RL方法在大规模问题上表现不佳，引入偏好优化、策略加速后性能提升显著（NIPS/ICML/ICLR/AAAI）

  - NCO中使用偏好优化不一定依赖“人类偏好”，更多是基于解质量构建偏好reward
  - 偏好优化不太需要人工标注标签

- 神经求解器解决组合优化问题的优势

  - 经过预训练的模型只需要微调，训练速度快
    - 主流做法：pretrain on synthetic CO problems → task-specific finetuning
    - 像基于Transformer、Diffusion、GNN的大模型都采用类似套路
  - 解的评估成本低
    - 当reward可以用可计算的目标函数直接计算时，评估成本低（常见的CO任务）
  - 采样式NCO+解修补模型+fine-tuning的策略会显著提高解质量（多样性+可行性），常超过经典启发式

- 未来可研究的方向

  - 大规模节点的经典组合优化问题求解：节点数>1000、>5000、>10000时，分块处理
  - 分布式训练NCO：训练需要大量问题实例，数据量大、多次采样、多节点训练
  - 工业级CO问题往往包含软硬混合约束（并行训练策略生成相关的组合优化问题）

  
